{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac4ca930580ea82",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data 3550 Final Project\n",
    "### Ayman Boules, Katherine Simon, Nicholas Sartino, Sammi Hamdan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e6373303a0bbb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9127c8924e6fad4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T04:37:44.134630Z",
     "start_time": "2023-11-17T04:37:44.131592Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Installs (comment out if you don't need)\n",
    "\n",
    "# !pip-install missingno\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ba825ea3a47b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T04:39:36.973631Z",
     "start_time": "2023-11-17T04:39:34.635837Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error,mean_absolute_percentage_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from numpy import arange\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "pd.set_option('display.max_columns',200) #allows for up to 200 columns to be displayed when viewing a dataframe\n",
    "pd.set_option('display.max_rows',100)\n",
    "# The following was commented out as it is deprecated and no longer works with current version.\n",
    "#plt.style.use('seaborn') # a style that can be used for plots - see style reference above\n",
    "\n",
    "# trick to widen the screen\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "#Widens the code landscape \n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc5c2be3272d77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T04:39:37.015582Z",
     "start_time": "2023-11-17T04:39:36.972962Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('College_Admission_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b940c0b3f7e093b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T04:39:40.872753Z",
     "start_time": "2023-11-17T04:39:40.866603Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ee5660b5e6fa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T04:40:05.296703Z",
     "start_time": "2023-11-17T04:40:05.098515Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb9cacb9698162",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T04:40:09.898488Z",
     "start_time": "2023-11-17T04:40:09.854858Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x='Carnegie Classification 2010: Basic',y='Enrolled total', data=df)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x='Carnegie Classification 2010: Basic', y='Applicants total', data=df)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and 'cat_column' is the column with categorical values\n",
    "distinct_values = df['Carnegie Classification 2010: Basic'].unique()\n",
    "\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"Baccalaureate/Associate's Colleges\": 1,\n",
    "    'Baccalaureate Colleges--Diverse Fields': 2,\n",
    "    'Baccalaureate Colleges--Arts & Sciences': 3,\n",
    "    \"Master's Colleges and Universities (smaller programs)\": 4,\n",
    "    \"Master's Colleges and Universities (medium programs)\": 5,\n",
    "    \"Master's Colleges and Universities (larger programs)\": 6,\n",
    "    'Research Universities (high research activity)': 7,\n",
    "    'Research Universities (very high research activity)': 8,\n",
    "    'Doctoral/Research Universities': 9\n",
    "}\n",
    "df['Carnegie Classification 2010: Basic'] = df['Carnegie Classification 2010: Basic'].replace(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and 'cat_column' is the column with categorical values\n",
    "distinct_values = df['Carnegie Classification 2010: Basic'].unique()\n",
    "\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary colums\n",
    "df.drop(['State abbreviation','FIPS state code','Geographic region'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the target variable and features\n",
    "X = df.drop('Carnegie Classification 2010: Basic', axis=1)\n",
    "y = df['Carnegie Classification 2010: Basic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categorical target variable into a numerical variable.\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Convert the encoded labels back to the original categories\n",
    "# y = le.inverse_transform(y_encoded)\n",
    "\n",
    "# Convert categorical features into numerical features.\n",
    "df_dummies = pd.get_dummies(X, drop_first=True)\n",
    "df_dummies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with missing values and their counts\n",
    "missing_counts = df_dummies.isnull().sum()\n",
    "\n",
    "# Filter and print columns with missing values\n",
    "columns_with_missing_values = missing_counts[missing_counts > 0]\n",
    "print(columns_with_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imputation\n",
    "* There are a lot of variables with missing values. Many of them are correlated as well. For example, all of the enrollment variables have exactly 2 missing values.\n",
    "* The goal in this section will be to try to maintain the representation of the data, which means that no statistical value will be used to fill in an NA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rules that will be followed:\n",
    "* columns having at least half of its values being missing values will be dropped.\n",
    "* columns having a significant number of missing values (>=100) will have its values substituted with the median (assuming a normal distribution).\n",
    "* columns columns having an insignificant number of missing values will be substituted with appropriate values that indicate that those values were not given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing columns with a very large amount of missing values\n",
    "\n",
    "# Threshold\n",
    "prop = 0.5\n",
    "\n",
    "# Drop columns with more than 'prop' proportion of missing values\n",
    "df_filtered = df_dummies.dropna(thresh=int(df_dummies.shape[0] * prop), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling columns having a significant number of missing values\n",
    "\n",
    "# Threshold\n",
    "min_missing_values = 100\n",
    "\n",
    "# Go through each column in the DataFrame\n",
    "for col in df_filtered.columns:\n",
    "    # If the column has 100 or more missing values\n",
    "    if df_filtered[col].isna().sum() >= min_missing_values:\n",
    "        # Fill the missing values with the median of the column\n",
    "        df_filtered[col].fillna(df_filtered[col].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling columns having an insignificant number of missing values.\n",
    "\n",
    "# Fill in the rest of the missing values with -1.\n",
    "df_filtered.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with missing values and their counts\n",
    "missing_counts = df_filtered.isnull().sum()\n",
    "\n",
    "# Filter and print columns with missing values\n",
    "columns_with_missing_values = missing_counts[missing_counts > 0]\n",
    "print(columns_with_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_filtered,y_encoded,test_size=0.3,random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Section\n",
    "* Categorical variables were converted into numerical variables.\n",
    "* Missing values were handled according to the rules determined.\n",
    "* The data was split into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6a67a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries.\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "\n",
    "#define the scalers\n",
    "sc = StandardScaler()\n",
    "mm = MinMaxScaler()\n",
    "rs = RobustScaler()\n",
    "nm = Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the data\n",
    "\n",
    "sc.fit(X_train)\n",
    "\n",
    "X_train_sc = sc.transform(X_train)\n",
    "X_train_sc = pd.DataFrame(X_train_sc, columns = X_train.columns)\n",
    "\n",
    "X_test_sc = sc.transform(X_test)\n",
    "X_test_sc = pd.DataFrame(X_test_sc, columns = X_test.columns)\n",
    "\n",
    "#MinMax scale  the data\n",
    "mm.fit(X_train)\n",
    "\n",
    "X_train_mm = mm.transform(X_train)\n",
    "X_train_mm = pd.DataFrame(X_train_mm, columns = X_train.columns)\n",
    "\n",
    "X_test_mm = mm.transform(X_test)\n",
    "X_test_mm = pd.DataFrame(X_test_mm, columns = X_test.columns)\n",
    "\n",
    "#Robust scale the data\n",
    "rs.fit(X_train)\n",
    "\n",
    "X_train_rs = rs.transform(X_train)\n",
    "X_train_rs = pd.DataFrame(X_train_rs, columns = X_train.columns)\n",
    "\n",
    "X_test_rs = rs.transform(X_test)\n",
    "X_test_rs = pd.DataFrame(X_test_rs, columns = X_test.columns)\n",
    "\n",
    "#Normalize the data\n",
    "nm.fit(X_train)\n",
    "\n",
    "X_train_nm = nm.transform(X_train)\n",
    "X_train_nm = pd.DataFrame(X_train_nm, columns = X_train.columns)\n",
    "\n",
    "X_test_nm = nm.transform(X_test)\n",
    "X_test_nm = pd.DataFrame(X_test_nm, columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use grid search to find the best parameters for the logistic regression model.\n",
    "# grid={\"C\":[0.001, 0.01, 0.1, 0.5, 1, 2, 5, 10], \"penalty\":[\"l1\",\"l2\"],\n",
    "#      \"class_weight\": [None, 'balanced', {0:1, 1:1.5}, {0:1, 1:2}, {0:1, 1:2.5}, {0:1, 1:3}, {0:1, 1:5}], \"solver\":['lbfgs', 'liblinear']}\n",
    "# logreg=LogisticRegression(random_state = 42,max_iter=10000)\n",
    "# logreg_cv=GridSearchCV(logreg,grid,cv=10)\n",
    "# logreg_cv.fit(X_train,y_train)\n",
    "\n",
    "# print(\"Tuned hyperparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "# print(\"Accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f6568",
   "metadata": {},
   "source": [
    "Above grid search took over 5 hours to run. To preserve notebook performance, I'm commenting the code block and pasting the results here.\n",
    "\n",
    "Result of above:\n",
    "Tuned hyperparameters :(best parameters)  {'C': 0.5, 'class_weight': None, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
    "Accuracy : 0.6001730702665282\n",
    "\n",
    "The non-scaled data performed better than the scaled data for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeltraintest(vartrain, vartest, y_train, y_test, model):\n",
    "\n",
    "    #Fit the model\n",
    "    model.fit(vartrain, y_train)\n",
    "\n",
    "    #Predict with the model\n",
    "    model_pred = model.predict(vartest)\n",
    "    model_prob = model.predict_proba(vartest)\n",
    "\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, model_pred))\n",
    "    print(\"\")\n",
    "\n",
    "    #Assess with the model\n",
    "    score = model.score(vartest, y_test)\n",
    "    score_format = 'Accuracy Score: {0:.4f}'.format(score)\n",
    "    print(score_format)\n",
    "\n",
    "    recall = recall_score(y_test, model_pred, average='weighted')\n",
    "    recall_format = 'Recall(Sensitivity) Score: {0:.4f}'.format(recall)\n",
    "    print(recall_format)\n",
    "    \n",
    "    precision = precision_score(y_test, model_pred, average='weighted')\n",
    "    precision_format = 'Precision(PPV) Score: {0:.4f}'.format(precision)\n",
    "    print(precision_format)\n",
    "        \n",
    "    f1 = f1_score(y_test, model_pred, average='weighted')\n",
    "    f1_format = 'F1 Score: {0:.4f}'.format(f1)\n",
    "    print(f1_format)\n",
    "    \n",
    "    # calculate roc curve\n",
    "    # y_pred_prob = model.predict_proba(vartest)[:,1]\n",
    "    # fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "    # roc_auc = roc_auc_score(y_test, y_pred_prob, average='weighted')\n",
    "    # roc_auc_format = 'ROC AUC Score: {0:.4f}'.format(roc_auc)\n",
    "    # print(roc_auc_format)\n",
    "    # print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model=LogisticRegression(C= 0.5,class_weight= None, penalty= 'l2', solver= 'lbfgs',random_state = 42,max_iter=200000,multi_class='ovr')\n",
    "# final_logreg = final_model.fit(X_train, y_train)\n",
    "\n",
    "modeltraintest(X_train,X_test,y_train,y_test,final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for each class\n",
    "y_probs = final_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "# Convert labels to one-hot encoding for ROC AUC calculation\n",
    "y_test_bin = label_binarize(y_test, classes=[0, 1, 2,3,4,5,6,7,8])\n",
    "\n",
    "# Calculate ROC AUC score for each class\n",
    "auc_scores = []\n",
    "for i in range(final_model.classes_.size):\n",
    "    auc = roc_auc_score(y_test_bin[:, i], y_probs[:, i])\n",
    "    auc_scores.append(auc)\n",
    "    print(f\"Class {i} ROC AUC Score: {auc:.4f}\")\n",
    "\n",
    "# Average ROC AUC score across all classes\n",
    "average_auc = sum(auc_scores) / len(auc_scores)\n",
    "print(f\"\\nAverage ROC AUC Score: {average_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred = final_model.predict(X_test)\n",
    "logreg_scores = []\n",
    "logreg_scores.append(final_model.score(X_test, y_test))\n",
    "logreg_scores.append(recall_score(y_test, model_pred, average='weighted'))\n",
    "logreg_scores.append(precision_score(y_test, model_pred, average='weighted'))\n",
    "for i in range(final_model.classes_.size):\n",
    "    auc = roc_auc_score(y_test_bin[:, i], y_probs[:, i])\n",
    "    logreg_scores.append(auc)\n",
    "logreg_scores\n",
    "# logreg_acc = final_model.score(X_test, y_test)\n",
    "# logreg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartrain = X_train_sc\n",
    "vartest = X_test_sc\n",
    "modeltraintest(vartrain,vartest,y_train,y_test,final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartrain = X_train_rs\n",
    "vartest = X_test_rs\n",
    "modeltraintest(vartrain,vartest,y_train,y_test,final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartrain = X_train_mm\n",
    "vartest = X_test_mm\n",
    "modeltraintest(vartrain,vartest,y_train,y_test,final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartrain = X_train_nm\n",
    "vartest = X_test_nm\n",
    "modeltraintest(vartrain,vartest,y_train,y_test,final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the important features\n",
    "coefficients = final_model.coef_[0]\n",
    "\n",
    "feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': np.abs(coefficients)})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=True).head(20)\n",
    "feature_importance.plot(x='Feature', y='Importance', kind='barh', figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 20 features.\n",
    "top_20 = feature_importance.sort_values('Importance', axis=0, ascending=False).head(20)\n",
    "top_20 = top_20.index\n",
    "top_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns based on indices\n",
    "selected_columns = X_train.iloc[:, top_20]\n",
    "print(selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data to include only important features\n",
    "X_important_train = X_train.iloc[:, top_20]\n",
    "X_important_test = X_test.iloc[:, top_20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the logisitic regression model using the top 20 important features.\n",
    "\n",
    "modeltraintest(X_important_train,X_important_test,y_train,y_test,final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "limiting to the top 20 most important features resulted in a worse outcome for the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df['Carnegie Classification 2010: Basic'].unique()\n",
    "le.inverse_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_lookup(dictionary, value):\n",
    "    keys = [key for key, val in dictionary.items() if val == value]\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reverse_lookup(mapping, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(final_model.classes_)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients and intercept\n",
    "coefficients = final_model.coef_\n",
    "intercept = final_model.intercept_\n",
    "num_classes = len(final_model.classes_)\n",
    "\n",
    "# Print the logistic equation\n",
    "print(\"Logistic Equation:\")\n",
    "\n",
    "for i in range(num_classes):\n",
    "    b = 0\n",
    "    class_equation = f\"Class {i}: \"\n",
    "    class_equation += f\"{intercept[i]:.4f}\"\n",
    "    for sub in coefficients[i][:]:\n",
    "        class_equation += f\" + ({coefficients[i][b]:.4f} * x{b+1})\"\n",
    "        b += 1\n",
    "    print(class_equation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee3b63",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c0084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Drop 'Name' column from X_train and X_test\n",
    "X_train = X_train.drop('Name', axis=1)\n",
    "X_test = X_test.drop('Name', axis=1)\n",
    "\n",
    "print(X_train.dtypes)\n",
    "print(X_test.dtypes)\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# If categorical columns are present, encode them using pd.get_dummies()\n",
    "if categorical_columns:\n",
    "    X_train_encoded = pd.get_dummies(X_train, columns=categorical_columns, drop_first=True)\n",
    "    X_test_encoded = pd.get_dummies(X_test, columns=categorical_columns, drop_first=True)\n",
    "    \n",
    "    # Display information about the encoded data\n",
    "    X_train_encoded.info()\n",
    "    X_test_encoded.info()\n",
    "\n",
    "    # Ensure both X_train_encoded and X_test_encoded have the same columns\n",
    "    common_columns = list(set(X_train_encoded.columns) & set(X_test_encoded.columns))\n",
    "    X_train_final = X_train_encoded[common_columns]\n",
    "    X_test_final = X_test_encoded[common_columns]\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e37078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train_final, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test_final)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Get a classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the trained Decision Tree Classifier\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# Create a DataFrame to associate feature names with their importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train_final.columns, 'Importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the top N most important features\n",
    "top_n = 10  # Set the number of top features you want to display\n",
    "top_features = feature_importance_df.head(top_n)\n",
    "print(top_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9750a",
   "metadata": {},
   "source": [
    "Based on the code above, the top ten features deamed important by the Decision Tree Classifier are outputted. It shows that among all the features, the \"Estimated graduate enrollment, total\" is the most significant feature with a importance value of 0.127.  \n",
    "\n",
    "Estimated graduate enrollment, total: This feature might have high importance because it could be strongly correlated or indicative of certain types of institutions that fall into specific Carnegie classifications. Institutions with higher graduate enrollment might exhibit characteristics associated with certain categories within the classification.  \n",
    "\n",
    "SAT Critical Reading 25th percentile score: This could indicate the academic profile of students, which might align with the criteria used in the Carnegie classification system to differentiate between different types of institutions.  \n",
    "\n",
    "Endowment assets per FTE enrollment: Institutions full-time equivalent (FTE) enrollment might belong to a specific category within the Carnegie classification due to financial resources or institutional characteristics associated with these levels of funding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc8c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "\n",
    "# Fit the Decision Tree Classifier with max_depth=5\n",
    "clf = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "clf.fit(X_train_final, y_train)\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(clf, filled=True, feature_names=X_train_final.columns.tolist(), class_names=[str(c) for c in le.classes_])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a range of depths to search through\n",
    "param_grid = {'max_depth': range(1, 20)}  # Adjust the range as needed\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
    "grid_search.fit(X_train_final, y_train)\n",
    "\n",
    "# Get the best estimator and its parameters\n",
    "dt_best = grid_search.best_estimator_\n",
    "best_depth = dt_best.get_params()['max_depth']\n",
    "print(f\"Best tree depth: {best_depth}\")\n",
    "\n",
    "# Fit the best model on the entire training data\n",
    "dt_best.fit(X_train_final, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Plot the final decision tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(dt_best, filled=True, feature_names=X_train_final.columns.tolist(), class_names=[str(c) for c in le.classes_])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff111e",
   "metadata": {},
   "source": [
    "There seems to be no difference in the dt_best model and the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c19cb76",
   "metadata": {},
   "source": [
    "# Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=[0, 1, 2,3,4,5,6,7,8]\n",
    "perf_scores = ['Accuarcy','Sensitivity','Specificity']\n",
    "for i in range(len(classes)):\n",
    "    perf_scores.append(f\"Class {i} ROC AUC Score\")\n",
    "\n",
    "# print(perf_scores)\n",
    "\n",
    "df_scores = pd.DataFrame(logreg_scores,index=perf_scores,columns=['LogisticRegressionScores'])\n",
    "df_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
