{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac4ca930580ea82",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data 3550 Final Project\n",
    "### Ayman Boules, Katherine Simon, Nicholas Sartino, Sammi Hamdan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e6373303a0bbb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9127c8924e6fad4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T04:37:44.134630Z",
     "start_time": "2023-11-17T04:37:44.131592Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Installs (comment out if you don't need)\n",
    "\n",
    "# !pip-install missingno\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ba825ea3a47b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T04:39:36.973631Z",
     "start_time": "2023-11-17T04:39:34.635837Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error,mean_absolute_percentage_error\n",
    "\n",
    "from numpy import arange\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "pd.set_option('display.max_columns',200) #allows for up to 200 columns to be displayed when viewing a dataframe\n",
    "pd.set_option('display.max_rows',100)\n",
    "# The following was commented out as it is deprecated and no longer works with current version.\n",
    "#plt.style.use('seaborn') # a style that can be used for plots - see style reference above\n",
    "\n",
    "# trick to widen the screen\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "#Widens the code landscape \n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df1a1b7",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ae889",
   "metadata": {},
   "source": [
    "## Load in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc5c2be3272d77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T04:39:37.015582Z",
     "start_time": "2023-11-17T04:39:36.972962Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('College_Admission_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df9167a",
   "metadata": {},
   "source": [
    "## Data Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b940c0b3f7e093b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T04:39:40.872753Z",
     "start_time": "2023-11-17T04:39:40.866603Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ee5660b5e6fa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T04:40:05.296703Z",
     "start_time": "2023-11-17T04:40:05.098515Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb9cacb9698162",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T04:40:09.898488Z",
     "start_time": "2023-11-17T04:40:09.854858Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99107054",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x='Carnegie Classification 2010: Basic',y='Enrolled total', data=df)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c33a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x='Carnegie Classification 2010: Basic', y='Applicants total', data=df)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a29fe6",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fc6c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the target variable and features\n",
    "X = df.drop('Carnegie Classification 2010: Basic', axis=1)\n",
    "y = df['Carnegie Classification 2010: Basic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categorical target variable into a numerical variable.\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Convert the encoded labels back to the original categories\n",
    "# y = le.inverse_transform(y_encoded)\n",
    "\n",
    "# Convert categorical features into numerical features.\n",
    "df_dummies = pd.get_dummies(X, drop_first=True)\n",
    "df_dummies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7db333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary colums\n",
    "df.drop(['State abbreviation','FIPS state code','Geographic region'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75540b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with missing values and their counts\n",
    "missing_counts = df_dummies.isnull().sum()\n",
    "\n",
    "# Filter and print columns with missing values\n",
    "columns_with_missing_values = missing_counts[missing_counts > 0]\n",
    "print(columns_with_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb751b2",
   "metadata": {},
   "source": [
    "### Data Imputation\n",
    "* There are a lot of variables with missing values. Many of them are correlated as well. For example, all of the enrollment variables have exactly 2 missing values.\n",
    "* The goal in this section will be to try to maintain the representation of the data, which means that no statistical value will be used to fill in an NA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e611809",
   "metadata": {},
   "source": [
    "#### Rules that will be followed:\n",
    "* columns having at least half of its values being missing values will be dropped.\n",
    "* columns having a significant number of missing values (>=100) will have its values substituted with the median (assuming a normal distribution).\n",
    "* columns columns having an insignificant number of missing values will be substituted with appropriate values that indicate that those values were not given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed08e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing columns with a very large amount of missing values\n",
    "\n",
    "# Threshold\n",
    "prop = 0.5\n",
    "\n",
    "# Drop columns with more than 'prop' proportion of missing values\n",
    "df_filtered = df_dummies.dropna(thresh=int(df_dummies.shape[0] * prop), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7bddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling columns having a significant number of missing values\n",
    "\n",
    "# Threshold\n",
    "min_missing_values = 100\n",
    "\n",
    "# Go through each column in the DataFrame\n",
    "for col in df_filtered.columns:\n",
    "    # If the column has 100 or more missing values\n",
    "    if df_filtered[col].isna().sum() >= min_missing_values:\n",
    "        # Fill the missing values with the median of the column\n",
    "        df_filtered[col].fillna(df_filtered[col].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e06b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling columns having an insignificant number of missing values.\n",
    "\n",
    "# Fill in the rest of the missing values with -1.\n",
    "df_filtered.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e48366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with missing values and their counts\n",
    "missing_counts = df_filtered.isnull().sum()\n",
    "\n",
    "# Filter and print columns with missing values\n",
    "columns_with_missing_values = missing_counts[missing_counts > 0]\n",
    "print(columns_with_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41064be8",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_filtered,y_encoded,test_size=0.3,random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5b535f",
   "metadata": {},
   "source": [
    "### Summary of Section\n",
    "* Categorical variables were converted into numerical variables.\n",
    "* Missing values were handled according to the rules determined.\n",
    "* The data was split into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d148db2",
   "metadata": {},
   "source": [
    "# Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6a67a0",
   "metadata": {},
   "source": [
    "Run Logisitc Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64316cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries.\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558c6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Scalers\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "\n",
    "#define the scalers\n",
    "sc = StandardScaler()\n",
    "mm = MinMaxScaler()\n",
    "rs = RobustScaler()\n",
    "nm = Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb21fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the data\n",
    "\n",
    "sc.fit(X_train)\n",
    "\n",
    "X_train_sc = sc.transform(X_train)\n",
    "X_train_sc = pd.DataFrame(X_train_sc, columns = X_train.columns)\n",
    "\n",
    "X_test_sc = sc.transform(X_test)\n",
    "X_test_sc = pd.DataFrame(X_test_sc, columns = X_test.columns)\n",
    "\n",
    "#MinMax scale  the data\n",
    "mm.fit(X_train)\n",
    "\n",
    "X_train_mm = mm.transform(X_train)\n",
    "X_train_mm = pd.DataFrame(X_train_mm, columns = X_train.columns)\n",
    "\n",
    "X_test_mm = mm.transform(X_test)\n",
    "X_test_mm = pd.DataFrame(X_test_mm, columns = X_test.columns)\n",
    "\n",
    "#Robust scale the data\n",
    "rs.fit(X_train)\n",
    "\n",
    "X_train_rs = rs.transform(X_train)\n",
    "X_train_rs = pd.DataFrame(X_train_rs, columns = X_train.columns)\n",
    "\n",
    "X_test_rs = rs.transform(X_test)\n",
    "X_test_rs = pd.DataFrame(X_test_rs, columns = X_test.columns)\n",
    "\n",
    "#Normalize the data\n",
    "nm.fit(X_train)\n",
    "\n",
    "X_train_nm = nm.transform(X_train)\n",
    "X_train_nm = pd.DataFrame(X_train_nm, columns = X_train.columns)\n",
    "\n",
    "X_test_nm = nm.transform(X_test)\n",
    "X_test_nm = pd.DataFrame(X_test_nm, columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8124a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use grid search to find the best parameters for the logistic regression model.\n",
    "# grid={\"C\":[0.001, 0.01, 0.1, 0.5, 1, 2, 5, 10], \"penalty\":[\"l1\",\"l2\"],\n",
    "#      \"class_weight\": [None, 'balanced', {0:1, 1:1.5}, {0:1, 1:2}, {0:1, 1:2.5}, {0:1, 1:3}, {0:1, 1:5}], \"solver\":['lbfgs', 'liblinear']}\n",
    "# logreg=LogisticRegression(random_state = 42,max_iter=10000)\n",
    "# logreg_cv=GridSearchCV(logreg,grid,cv=10)\n",
    "# logreg_cv.fit(X_train,y_train)\n",
    "\n",
    "# print(\"Tuned hyperparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "# print(\"Accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c4a65a",
   "metadata": {},
   "source": [
    "Above grid search took over 5 hours to run. To preserve notebook performance, I'm commenting the code block and pasting the results here.\n",
    "\n",
    "Result of above:\n",
    "Tuned hyperparameters :(best parameters)  {'C': 0.5, 'class_weight': None, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
    "Accuracy : 0.6001730702665282\n",
    "\n",
    "The non-scaled data performed better than the scaled data for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to run the model on given data and print scores.\n",
    "def modeltraintest(vartrain, vartest, y_train, y_test, model):\n",
    "\n",
    "    #Fit the model\n",
    "    model.fit(vartrain, y_train)\n",
    "\n",
    "    #Predict with the model\n",
    "    model_pred = model.predict(vartest)\n",
    "    model_prob = model.predict_proba(vartest)\n",
    "\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, model_pred))\n",
    "    print(\"\")\n",
    "\n",
    "    #Assess with the model\n",
    "    score = model.score(vartest, y_test)\n",
    "    score_format = 'Accuracy Score: {0:.4f}'.format(score)\n",
    "    print(score_format)\n",
    "\n",
    "    recall = recall_score(y_test, model_pred, average='weighted')\n",
    "    recall_format = 'Recall(Sensitivity) Score: {0:.4f}'.format(recall)\n",
    "    print(recall_format)\n",
    "    \n",
    "    precision = precision_score(y_test, model_pred, average='weighted')\n",
    "    precision_format = 'Precision(PPV) Score: {0:.4f}'.format(precision)\n",
    "    print(precision_format)\n",
    "        \n",
    "    f1 = f1_score(y_test, model_pred, average='weighted')\n",
    "    f1_format = 'F1 Score: {0:.4f}'.format(f1)\n",
    "    print(f1_format)\n",
    "    \n",
    "    # calculate roc curve\n",
    "    # y_pred_prob = model.predict_proba(vartest)[:,1]\n",
    "    # fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "    # roc_auc = roc_auc_score(y_test, y_pred_prob, average='weighted')\n",
    "    # roc_auc_format = 'ROC AUC Score: {0:.4f}'.format(roc_auc)\n",
    "    # print(roc_auc_format)\n",
    "    # print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d002a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model=LogisticRegression(C= 0.5,class_weight= None, penalty= 'l2', solver= 'lbfgs',random_state = 42,max_iter=200000,multi_class='ovr')\n",
    "final_logreg = final_model.fit(X_train, y_train)\n",
    "\n",
    "modeltraintest(X_train,X_test,y_train,y_test,final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e98f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for each class\n",
    "y_probs = final_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4092bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "# Convert labels to one-hot encoding for ROC AUC calculation\n",
    "y_test_bin = label_binarize(y_test, classes=[0, 1, 2,3,4,5,6,7,8])\n",
    "\n",
    "# Calculate ROC AUC score for each class\n",
    "auc_scores = []\n",
    "for i in range(final_model.classes_.size):\n",
    "    auc = roc_auc_score(y_test_bin[:, i], y_probs[:, i])\n",
    "    auc_scores.append(auc)\n",
    "    print(f\"Class {i} ROC AUC Score: {auc:.4f}\")\n",
    "\n",
    "# Average ROC AUC score across all classes\n",
    "average_auc = sum(auc_scores) / len(auc_scores)\n",
    "print(f\"\\nAverage ROC AUC Score: {average_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of scores for the Logistic Regression model.\n",
    "model_pred = final_model.predict(X_test)\n",
    "logreg_scores = []\n",
    "logreg_scores.append(final_model.score(X_test, y_test))\n",
    "logreg_scores.append(recall_score(y_test, model_pred, average='weighted'))\n",
    "logreg_scores.append(precision_score(y_test, model_pred, average='weighted'))\n",
    "for i in range(final_model.classes_.size):\n",
    "    auc = roc_auc_score(y_test_bin[:, i], y_probs[:, i])\n",
    "    logreg_scores.append(auc)\n",
    "logreg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try standard scaled data on the model.\n",
    "vartrain = X_train_sc\n",
    "vartest = X_test_sc\n",
    "modeltraintest(vartrain,vartest,y_train,y_test,final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fa341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try robust scaled data on the model.\n",
    "vartrain = X_train_rs\n",
    "vartest = X_test_rs\n",
    "modeltraintest(vartrain,vartest,y_train,y_test,final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73281327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try minmax scaled data on the model.\n",
    "vartrain = X_train_mm\n",
    "vartest = X_test_mm\n",
    "modeltraintest(vartrain,vartest,y_train,y_test,final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a79e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try normalized data on the model.\n",
    "vartrain = X_train_nm\n",
    "vartest = X_test_nm\n",
    "modeltraintest(vartrain,vartest,y_train,y_test,final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b1374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the important features\n",
    "coefficients = final_model.coef_[0]\n",
    "\n",
    "feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': np.abs(coefficients)})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=True).head(20)\n",
    "feature_importance.plot(x='Feature', y='Importance', kind='barh', figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12743a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 20 features.\n",
    "top_20 = feature_importance.sort_values('Importance', axis=0, ascending=False).head(20)\n",
    "top_20 = top_20.index\n",
    "top_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d640ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns based on indices\n",
    "selected_columns = X_train.iloc[:, top_20]\n",
    "print(selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2eaccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data to include only important features\n",
    "X_important_train = X_train.iloc[:, top_20]\n",
    "X_important_test = X_test.iloc[:, top_20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087dc4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the logisitic regression model using the top 20 important features.\n",
    "\n",
    "modeltraintest(X_important_train,X_important_test,y_train,y_test,final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e7f5b0",
   "metadata": {},
   "source": [
    "limiting to the top 20 most important features resulted in a worse outcome for the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902237f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients and intercept\n",
    "coefficients = final_model.coef_\n",
    "intercept = final_model.intercept_\n",
    "num_classes = len(final_model.classes_)\n",
    "\n",
    "# Print the logistic equation\n",
    "print(\"Logistic Equation:\")\n",
    "\n",
    "for i in range(num_classes):\n",
    "    b = 0\n",
    "    class_equation = f\"Class {i}: \"\n",
    "    class_equation += f\"{intercept[i]:.4f}\"\n",
    "    for sub in coefficients[i][:]:\n",
    "        class_equation += f\" + ({coefficients[i][b]:.4f} * x{b+1})\"\n",
    "        b += 1\n",
    "    print(class_equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee3b63",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c0084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Drop 'Name' column from X_train and X_test\n",
    "X_train = X_train.drop('Name', axis=1)\n",
    "X_test = X_test.drop('Name', axis=1)\n",
    "\n",
    "print(X_train.dtypes)\n",
    "print(X_test.dtypes)\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# If categorical columns are present, encode them using pd.get_dummies()\n",
    "if categorical_columns:\n",
    "    X_train_encoded = pd.get_dummies(X_train, columns=categorical_columns, drop_first=True)\n",
    "    X_test_encoded = pd.get_dummies(X_test, columns=categorical_columns, drop_first=True)\n",
    "    \n",
    "    # Display information about the encoded data\n",
    "    X_train_encoded.info()\n",
    "    X_test_encoded.info()\n",
    "\n",
    "    # Ensure both X_train_encoded and X_test_encoded have the same columns\n",
    "    common_columns = list(set(X_train_encoded.columns) & set(X_test_encoded.columns))\n",
    "    X_train_final = X_train_encoded[common_columns]\n",
    "    X_test_final = X_test_encoded[common_columns]\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e37078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train_final, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test_final)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Get a classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the trained Decision Tree Classifier\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# Create a DataFrame to associate feature names with their importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train_final.columns, 'Importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the top N most important features\n",
    "top_n = 10  # Set the number of top features you want to display\n",
    "top_features = feature_importance_df.head(top_n)\n",
    "print(top_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9750a",
   "metadata": {},
   "source": [
    "Based on the code above, the top ten features deamed important by the Decision Tree Classifier are outputted. It shows that among all the features, the \"Estimated graduate enrollment, total\" is the most significant feature with a importance value of 0.127.  \n",
    "\n",
    "Estimated graduate enrollment, total: This feature might have high importance because it could be strongly correlated or indicative of certain types of institutions that fall into specific Carnegie classifications. Institutions with higher graduate enrollment might exhibit characteristics associated with certain categories within the classification.  \n",
    "\n",
    "SAT Critical Reading 25th percentile score: This could indicate the academic profile of students, which might align with the criteria used in the Carnegie classification system to differentiate between different types of institutions.  \n",
    "\n",
    "Endowment assets per FTE enrollment: Institutions full-time equivalent (FTE) enrollment might belong to a specific category within the Carnegie classification due to financial resources or institutional characteristics associated with these levels of funding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc8c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "\n",
    "# Fit the Decision Tree Classifier with max_depth=5\n",
    "clf = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "clf.fit(X_train_final, y_train)\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(clf, filled=True, feature_names=X_train_final.columns.tolist(), class_names=[str(c) for c in le.classes_])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a range of depths to search through\n",
    "param_grid = {'max_depth': range(1, 20)}  # Adjust the range as needed\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
    "grid_search.fit(X_train_final, y_train)\n",
    "\n",
    "# Get the best estimator and its parameters\n",
    "dt_best = grid_search.best_estimator_\n",
    "best_depth = dt_best.get_params()['max_depth']\n",
    "print(f\"Best tree depth: {best_depth}\")\n",
    "\n",
    "# Fit the best model on the entire training data\n",
    "dt_best.fit(X_train_final, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Plot the final decision tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(dt_best, filled=True, feature_names=X_train_final.columns.tolist(), class_names=[str(c) for c in le.classes_])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff111e",
   "metadata": {},
   "source": [
    "There seems to be no difference in the dt_best model and the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c37172",
   "metadata": {},
   "source": [
    "# Step 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table to hold the scores for each model.\n",
    "classes=[0, 1, 2,3,4,5,6,7,8]\n",
    "perf_scores = ['Accuarcy','Sensitivity','Specificity']\n",
    "for i in range(len(classes)):\n",
    "    perf_scores.append(f\"Class {i} ROC AUC Score\")\n",
    "\n",
    "# print(perf_scores)\n",
    "\n",
    "df_scores = pd.DataFrame(logreg_scores,index=perf_scores,columns=['LogisticRegressionScores'])\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4777f8",
   "metadata": {},
   "source": [
    "# Step 6: Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d643db",
   "metadata": {},
   "source": [
    "## Preprocessing For Regression Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e77c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_focused = df.dropna(subset=['Percent admitted - total'])\n",
    "\n",
    "# Create A regression model to analyze the admission rate based on factors such as the number of applicants, tuition, average SAT and ACT scores, and other relevant metricds we find.\n",
    "\n",
    "# Admission rate = total admissions / applicants total\n",
    "\n",
    "X2 = df_focused.drop('Percent admitted - total',axis=1)\n",
    "y2 = df_focused['Percent admitted - total']\n",
    "\n",
    "df_dummies = pd.get_dummies(X2, drop_first=True)\n",
    "df_dummies.info()\n",
    "\n",
    "# Removing columns with a very large amount of missing values\n",
    "\n",
    "# Threshold\n",
    "prop = 0.5\n",
    "\n",
    "# Drop columns with more than 'prop' proportion of missing values\n",
    "df_filtered = df_dummies.dropna(thresh=int(df_dummies.shape[0] * prop), axis=1)\n",
    "\n",
    "\n",
    "# Handling columns having a significant number of missing values\n",
    "\n",
    "# Threshold\n",
    "min_missing_values = 100\n",
    "\n",
    "# Go through each column in the DataFrame\n",
    "for col in df_filtered.columns:\n",
    "    # If the column has 100 or more missing values\n",
    "    if df_filtered[col].isna().sum() >= min_missing_values:\n",
    "        # Fill the missing values with the median of the column\n",
    "        df_filtered[col].fillna(df_filtered[col].median(), inplace=True)\n",
    "\n",
    "\n",
    "# Handling columns having an insignificant number of missing values.\n",
    "\n",
    "# Fill in the rest of the missing values with -1.\n",
    "df_filtered.fillna(-1, inplace=True)\n",
    "\n",
    "# Find columns with missing values and their counts\n",
    "missing_counts = df_filtered.isnull().sum()\n",
    "\n",
    "# Filter and print columns with missing values\n",
    "columns_with_missing_values = missing_counts[missing_counts > 0]\n",
    "print(columns_with_missing_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3defe230",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd4a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_filtered,y2,test_size=0.3,random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_focused = df.dropna(subset=['Percent admitted - total'])\n",
    "\n",
    "# Create A regression model to analyze the admission rate based on factors such as the number of applicants, tuition, average SAT and ACT scores, and other relevant metricds we find.\n",
    "\n",
    "# Admission rate = total admissions / applicants total\n",
    "\n",
    "X2 = df_focused.drop('Percent admitted - total',axis=1)\n",
    "y2 = df_focused['Percent admitted - total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76606633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "# fit the data i.e calculate the mean and sd of each coluns of X_train\n",
    "sc.fit(X_train)\n",
    "\n",
    "#transform the train data\n",
    "X_train_sc = sc.transform(X_train)\n",
    "X_train_sc = pd.DataFrame(X_train_sc, columns=X_train.columns)\n",
    "\n",
    "#transform the test data\n",
    "X_test_sc = sc.transform(X_test)\n",
    "X_test_sc = pd.DataFrame(X_test_sc, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define the range of lambda (alpha) values you want to test\n",
    "param_grid = {'alpha': [0.00000000001, 0.00000001,0.0000001, 0.000001, 0.00001, 0.0001, 0.0005, 0.001, 0.01, 0.1, 0.5, 1, 5, 10, 50, 100, 1000]}\n",
    "\n",
    "grid_search = GridSearchCV(lasso, param_grid, scoring='neg_mean_squared_error', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c159666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the grid search\n",
    "grid_search.fit(X_train_sc, y_train)\n",
    "\n",
    "# Find the best lambda (alpha) value\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "\n",
    "# Train a Ridge model with the best lambda value\n",
    "best_lasso_model = Lasso(alpha=best_alpha)\n",
    "best_lasso_model.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe3b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the estimated y values using the test dataset\n",
    "y_hat_test = best_lasso_model.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d73004",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = mean_squared_error(y_test, y_hat_test)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_hat_test)\n",
    "test_r_squared = r2_score(y_test, y_hat_test)\n",
    "test_mape = mean_absolute_percentage_error(y_test, y_hat_test)\n",
    "\n",
    "print(f'The test RMSE is:\\t{round(test_rmse, 3)}')\n",
    "print(f'The test Rsquared is:\\t{round(test_r_squared, 4)}')\n",
    "print(f'The test MAPE is:\\t{round(test_mape, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f674c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = [actual - predicted for actual, predicted in zip(y_test, y_hat_test)]\n",
    "# Create a scatter plot\n",
    "plt.scatter(y_test, residuals)\n",
    "plt.xlabel('Percent admitted - total')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c419c0af",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c19cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_focused = df.dropna(subset=['Percent admitted - total'])\n",
    "\n",
    "# Create A regression model to analyze the admission rate based on factors such as the number of applicants, tuition, average SAT and ACT scores, and other relevant metricds we find.\n",
    "\n",
    "# Admission rate = total admissions / applicants total\n",
    "\n",
    "X2 = df_focused.drop('Percent admitted - total',axis=1)\n",
    "y2 = df_focused['Percent admitted - total']\n",
    "\n",
    "df_dummies = pd.get_dummies(X2, drop_first=True)\n",
    "df_dummies.info()\n",
    "\n",
    "# Removing columns with a very large amount of missing values\n",
    "\n",
    "# Threshold\n",
    "prop = 0.5\n",
    "\n",
    "# Drop columns with more than 'prop' proportion of missing values\n",
    "df_filtered = df_dummies.dropna(thresh=int(df_dummies.shape[0] * prop), axis=1)\n",
    "\n",
    "\n",
    "# Handling columns having a significant number of missing values\n",
    "\n",
    "# Threshold\n",
    "min_missing_values = 100\n",
    "\n",
    "# Go through each column in the DataFrame\n",
    "for col in df_filtered.columns:\n",
    "    # If the column has 100 or more missing values\n",
    "    if df_filtered[col].isna().sum() >= min_missing_values:\n",
    "        # Fill the missing values with the median of the column\n",
    "        df_filtered[col].fillna(df_filtered[col].median(), inplace=True)\n",
    "\n",
    "\n",
    "# Handling columns having an insignificant number of missing values.\n",
    "\n",
    "# Fill in the rest of the missing values with -1.\n",
    "df_filtered.fillna(-1, inplace=True)\n",
    "\n",
    "# Find columns with missing values and their counts\n",
    "missing_counts = df_filtered.isnull().sum()\n",
    "\n",
    "# Filter and print columns with missing values\n",
    "columns_with_missing_values = missing_counts[missing_counts > 0]\n",
    "print(columns_with_missing_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafab31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rid, X_test_rid, y_train_rid, y_test_rid = train_test_split(df_filtered,y2,test_size=0.3,random_state=21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44418e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge() \n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "# fit the data i.e calculate the mean and sd of each coluns of X_train\n",
    "sc.fit(X_train_rid)\n",
    "\n",
    "#transform the train data\n",
    "X_train_sc = sc.transform(X_train_rid)\n",
    "X_train_sc = pd.DataFrame(X_train_sc, columns=X_train_rid.columns)\n",
    "\n",
    "#transform the test data\n",
    "X_test_sc = sc.transform(X_test_rid)\n",
    "X_test_sc = pd.DataFrame(X_test_sc, columns=X_test_rid.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcfd927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Define the parameter grid for Ridge regression\n",
    "param_grid = {'alpha': [0.001,0.1, 1, 10, 100,1000, 10000]} \n",
    "\n",
    "# Initialize Ridge regression\n",
    "ridge = Ridge()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid)\n",
    "grid_search.fit(X_train_sc, y_train_rid)\n",
    "\n",
    "# Find the best lambda (alpha) value\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "\n",
    "# Train a Ridge model with the best lambda value\n",
    "best_ridge_model = Ridge(alpha=best_alpha)\n",
    "best_ridge_model.fit(X_train_sc, y_train_rid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc86f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the estimated y values using the test dataset\n",
    "y_hat_test = best_ridge_model.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = mean_squared_error(y_test_rid, y_hat_test)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test_rid, y_hat_test)\n",
    "test_r_squared = r2_score(y_test_rid, y_hat_test)\n",
    "test_mape = mean_absolute_percentage_error(y_test_rid, y_hat_test)\n",
    "\n",
    "print(f'The test RMSE is:\\t{round(test_rmse, 3)}')\n",
    "print(f'The test Rsquared is:\\t{round(test_r_squared, 4)}')\n",
    "print(f'The test MAPE is:\\t{round(test_mape, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96388a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.0000001,0.001,0.1, 1, 10, 100,],\n",
    "    'kernel': ['poly','laplacian', 'rbf', 'sigmoid'],\n",
    "    'gamma': [0.0000001, 1,5, 10, 15]  \n",
    "}\n",
    "\n",
    "# Standardize your features if needed\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train_rid)  # Assuming X_train is your feature matrix\n",
    "\n",
    "# Initialize Kernel Ridge regression\n",
    "krr = KernelRidge()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=krr, param_grid=param_grid)\n",
    "grid_search.fit(X_train_sc, y_train_rid)\n",
    "\n",
    "# Find the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train a Kernel Ridge model with the best parameters\n",
    "best_krr_model = KernelRidge(**best_params)\n",
    "best_krr_model.fit(X_train_sc, y_train_rid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460ac994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Assuming you have a test dataset X_test and y_test\n",
    "X_test_sc = scaler.transform(X_test_rid)  # Scale the test features\n",
    "\n",
    "# Make predictions using the best Kernel Ridge model\n",
    "y_pred = best_krr_model.predict(X_test_sc)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test_rid, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "# Calculate R-squared (R2)\n",
    "r2 = r2_score(y_test_rid, y_pred)\n",
    "print(f\"R-squared (R2): {r2}\")\n",
    "\n",
    "\n",
    "cv_scores = cross_val_score(best_krr_model, X_train_sc, y_train_rid, cv=5, scoring='r2')\n",
    "print(\"Cross-Validation R-squared scores:\", cv_scores)\n",
    "print(f\"Mean Cross-Validation R-squared: {cv_scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5f6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
